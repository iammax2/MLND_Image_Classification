{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CIFAR-10 Dataset: 171MB [00:41, 4.13MB/s]                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 4:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1003, 1: 963, 2: 1041, 3: 976, 4: 1004, 5: 1021, 6: 1004, 7: 981, 8: 1024, 9: 983}\n",
      "First 20 Labels: [0, 6, 0, 2, 7, 2, 1, 2, 4, 1, 5, 6, 6, 3, 1, 3, 5, 5, 8, 1]\n",
      "\n",
      "Example of Image 559:\n",
      "Image - Min Value: 18 Max Value: 241\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 4 Name: deer\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xf6eb78b2b0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 4\n",
    "sample_id = 559\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return x / 255\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "Look into LabelBinarizer in the preprocessing module of sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    L_binar = preprocessing.LabelBinarizer()\n",
    "    L_binar.fit([0,1,2,3,4,5,6,7,8,9])\n",
    "    return L_binar.transform(x)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(dtype= tf.float32, shape = [None, image_shape[0], image_shape[1], image_shape[2]], name = \"x\")\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(dtype = tf.float32, shape = [None, n_classes], name = \"y\")\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(dtype = tf.float32, name = \"keep_prob\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers.\n",
    "\n",
    "** Hint: **\n",
    "\n",
    "When unpacking values as an argument in Python, look into the [unpacking](https://docs.python.org/3/tutorial/controlflow.html#unpacking-argument-lists) operator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    weight = tf.Variable(tf.truncated_normal(shape = [conv_ksize[0], conv_ksize[1], x_tensor.get_shape().as_list()[3], conv_num_outputs],\n",
    "                                             mean = 0.0, \n",
    "                                             stddev = 1.0/conv_num_outputs, \n",
    "                                             seed = 42))\n",
    "    \n",
    "    bias = tf.Variable(tf.zeros(shape = conv_num_outputs))\n",
    "    \n",
    "    conv_layer = tf.nn.bias_add(tf.nn.conv2d(x_tensor, \n",
    "                                             weight, \n",
    "                                             strides = [1, conv_strides[0], conv_strides[1], 1], \n",
    "                                             padding = \"SAME\"),\n",
    "                               bias)\n",
    "    \n",
    "    act_funct = tf.nn.relu(conv_layer)\n",
    "    \n",
    "    max_pool = tf.nn.max_pool(act_funct, \n",
    "                   ksize = [1, pool_ksize[0], pool_ksize[1], 1], \n",
    "                   strides = [1, pool_strides[0], pool_strides[1], 1], \n",
    "                   padding = \"SAME\")\n",
    "    \n",
    "    return max_pool\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    shape_of_tensor = x_tensor.get_shape().as_list()\n",
    "    flatten_shape = shape_of_tensor[1] * shape_of_tensor[2] * shape_of_tensor[3]\n",
    "    return tf.reshape(x_tensor, shape = [-1, flatten_shape]) # the -1 keeps that deminsion constant, in this case that deminsion is\n",
    "    # batch size which we want to remain the same after reshaping it\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    weight = tf.Variable(tf.truncated_normal(shape = [x_tensor.get_shape().as_list()[1], num_outputs], \n",
    "                                            mean = 0.0,\n",
    "                                            stddev = 1.0/num_outputs,\n",
    "                                            seed = 42))\n",
    "    \n",
    "    bais = tf.Variable(tf.zeros(shape = num_outputs))\n",
    "    conv = tf.add(tf.matmul(x_tensor, weight), bais)\n",
    "    act_funct = tf.nn.relu(conv)\n",
    "    \n",
    "    return act_funct\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    weight = tf.Variable(tf.truncated_normal(shape = [x_tensor.get_shape().as_list()[1], num_outputs],\n",
    "                                            mean = 0.0,\n",
    "                                            stddev = 1.0/num_outputs,\n",
    "                                            seed = 42))\n",
    "    \n",
    "    bais = tf.Variable(tf.zeros(shape = num_outputs))\n",
    "    \n",
    "    output = tf.add(tf.matmul(x_tensor, weight), bais)\n",
    "    \n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "    con_mpl_1 = conv2d_maxpool(x, 32, conv_ksize=[7,7], conv_strides=[3,3], pool_ksize=[2,2], pool_strides=[1,1])\n",
    "    drop1 = tf.nn.dropout(con_mpl_1, keep_prob)\n",
    "    \n",
    "    con_mpl_2 = conv2d_maxpool(drop1, 64, conv_ksize=[7,7], conv_strides=[3,3], pool_ksize=[2,2], pool_strides=[1,1])\n",
    "    drop2 = tf.nn.dropout(con_mpl_2, keep_prob)\n",
    "    \n",
    "    con_mpl_3 = conv2d_maxpool(drop2, 128, conv_ksize=[7,7], conv_strides=[3,3], pool_ksize=[2,2], pool_strides=[1,1])\n",
    "    drop3 = tf.nn.dropout(con_mpl_3, keep_prob)\n",
    "    \n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    \n",
    "    flat = flatten(drop3)\n",
    "    \n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    \n",
    "    conv_fully_conn_1 = fully_conn(flat, 20)\n",
    "    ful_drop_1 = tf.nn.dropout(conv_fully_conn_1, keep_prob)\n",
    "    \n",
    "    conv_fully_conn_2 = fully_conn(ful_drop_1, 20)\n",
    "    ful_drop_2 = tf.nn.dropout(conv_fully_conn_2, keep_prob)\n",
    "    \n",
    "    conv_fully_conn_3 = fully_conn(ful_drop_2, 20)\n",
    "    ful_drop_3 = tf.nn.dropout(conv_fully_conn_3, keep_prob)\n",
    "    \n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    \n",
    "    out = output(ful_drop_3, 10)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return out\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return session.run(optimizer, feed_dict = {x : feature_batch, \n",
    "                                              y : label_batch,\n",
    "                                              keep_prob : keep_probability})\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    loss = session.run(cost, feed_dict = {x : feature_batch,\n",
    "                                       y : label_batch,\n",
    "                                       keep_prob: 1.0})\n",
    "    accuracy = session.run(accuracy, feed_dict = {x : valid_features,\n",
    "                                                 y : valid_labels,\n",
    "                                                 keep_prob: 1.0})\n",
    "    \n",
    "    print(\"Loss = \", loss, \"   Accuracy = \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 150\n",
    "batch_size = 512\n",
    "keep_probability = .85"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss =  2.29728    Accuracy =  0.139\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss =  2.2258    Accuracy =  0.1686\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss =  2.15848    Accuracy =  0.1912\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss =  2.11299    Accuracy =  0.1694\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss =  2.11251    Accuracy =  0.1858\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss =  2.07892    Accuracy =  0.19\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss =  2.0744    Accuracy =  0.1972\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss =  2.02494    Accuracy =  0.2106\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss =  1.99164    Accuracy =  0.2196\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss =  1.96534    Accuracy =  0.2298\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss =  1.94186    Accuracy =  0.2582\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss =  1.91529    Accuracy =  0.2724\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss =  1.89343    Accuracy =  0.2888\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss =  1.86674    Accuracy =  0.3014\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss =  1.84416    Accuracy =  0.3008\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss =  1.84339    Accuracy =  0.3116\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss =  1.79801    Accuracy =  0.3124\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss =  1.80513    Accuracy =  0.307\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss =  1.75512    Accuracy =  0.3302\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss =  1.75209    Accuracy =  0.3416\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss =  1.72137    Accuracy =  0.343\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss =  1.68851    Accuracy =  0.3598\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss =  1.66649    Accuracy =  0.3658\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss =  1.63799    Accuracy =  0.3672\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss =  1.61315    Accuracy =  0.3842\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss =  1.59292    Accuracy =  0.3928\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss =  1.55774    Accuracy =  0.3934\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss =  1.56411    Accuracy =  0.3842\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss =  1.53726    Accuracy =  0.3892\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss =  1.54664    Accuracy =  0.4096\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss =  1.49286    Accuracy =  0.3978\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss =  1.53029    Accuracy =  0.3886\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss =  1.42579    Accuracy =  0.423\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss =  1.40753    Accuracy =  0.4184\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss =  1.42093    Accuracy =  0.4198\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss =  1.35819    Accuracy =  0.437\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss =  1.34449    Accuracy =  0.4464\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss =  1.33144    Accuracy =  0.4464\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss =  1.36372    Accuracy =  0.4282\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss =  1.32442    Accuracy =  0.44\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss =  1.26496    Accuracy =  0.448\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss =  1.2472    Accuracy =  0.4574\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss =  1.24674    Accuracy =  0.4518\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss =  1.33088    Accuracy =  0.4226\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss =  1.2296    Accuracy =  0.4542\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss =  1.209    Accuracy =  0.446\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss =  1.18064    Accuracy =  0.4612\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss =  1.21366    Accuracy =  0.4452\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss =  1.20611    Accuracy =  0.4718\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss =  1.14819    Accuracy =  0.4598\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss =  1.10421    Accuracy =  0.4672\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss =  1.10333    Accuracy =  0.4742\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss =  1.06427    Accuracy =  0.4752\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss =  1.07159    Accuracy =  0.4646\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss =  1.09034    Accuracy =  0.474\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss =  1.05843    Accuracy =  0.4718\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss =  1.00167    Accuracy =  0.4792\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss =  0.998573    Accuracy =  0.4862\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss =  1.00481    Accuracy =  0.4816\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss =  0.981093    Accuracy =  0.4816\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss =  0.984636    Accuracy =  0.4662\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss =  0.978201    Accuracy =  0.4672\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss =  0.965016    Accuracy =  0.4662\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss =  1.1128    Accuracy =  0.4304\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss =  0.973877    Accuracy =  0.4756\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss =  0.942358    Accuracy =  0.4746\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss =  0.949873    Accuracy =  0.4814\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss =  0.98924    Accuracy =  0.4856\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss =  0.871122    Accuracy =  0.4914\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss =  0.805168    Accuracy =  0.5002\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss =  0.791807    Accuracy =  0.4928\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss =  0.787568    Accuracy =  0.4972\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss =  0.775199    Accuracy =  0.4984\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss =  0.743001    Accuracy =  0.5056\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss =  0.749445    Accuracy =  0.51\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss =  0.728617    Accuracy =  0.5098\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss =  0.718551    Accuracy =  0.5096\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss =  0.700415    Accuracy =  0.5174\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss =  0.70073    Accuracy =  0.5092\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss =  0.704484    Accuracy =  0.4988\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss =  0.704354    Accuracy =  0.4946\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss =  0.651799    Accuracy =  0.5184\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss =  0.618354    Accuracy =  0.5146\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss =  0.634227    Accuracy =  0.5022\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss =  0.621102    Accuracy =  0.5114\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss =  0.632812    Accuracy =  0.4976\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss =  0.585328    Accuracy =  0.5102\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss =  0.563097    Accuracy =  0.5196\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss =  0.607453    Accuracy =  0.5164\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss =  0.5884    Accuracy =  0.5086\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss =  0.589837    Accuracy =  0.4962\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss =  0.56259    Accuracy =  0.5014\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss =  0.550336    Accuracy =  0.5168\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss =  0.530582    Accuracy =  0.5116\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss =  0.49251    Accuracy =  0.5158\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss =  0.513617    Accuracy =  0.5\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss =  0.509464    Accuracy =  0.5044\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss =  0.538347    Accuracy =  0.4958\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss =  0.486972    Accuracy =  0.4982\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss =  0.475249    Accuracy =  0.4894\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss =  0.497707    Accuracy =  0.4948\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss =  0.456149    Accuracy =  0.509\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss =  0.500878    Accuracy =  0.4988\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss =  0.487732    Accuracy =  0.4912\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss =  0.428207    Accuracy =  0.5134\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss =  0.460048    Accuracy =  0.5092\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss =  0.434448    Accuracy =  0.5234\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss =  0.446771    Accuracy =  0.5012\n",
      "Epoch 109, CIFAR-10 Batch 1:  Loss =  0.467362    Accuracy =  0.494\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss =  0.537422    Accuracy =  0.4838\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss =  0.501418    Accuracy =  0.4958\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss =  0.414069    Accuracy =  0.5202\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss =  0.373046    Accuracy =  0.529\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss =  0.352025    Accuracy =  0.5262\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss =  0.324306    Accuracy =  0.5296\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss =  0.330555    Accuracy =  0.5236\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss =  0.33494    Accuracy =  0.5176\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss =  0.338318    Accuracy =  0.5218\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss =  0.323669    Accuracy =  0.5134\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss =  0.294051    Accuracy =  0.5228\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss =  0.300564    Accuracy =  0.5198\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss =  0.276821    Accuracy =  0.5416\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss =  0.27821    Accuracy =  0.5358\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss =  0.277447    Accuracy =  0.5294\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss =  0.263481    Accuracy =  0.5384\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss =  0.262972    Accuracy =  0.5338\n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss =  0.283729    Accuracy =  0.5298\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss =  0.259305    Accuracy =  0.535\n",
      "Epoch 129, CIFAR-10 Batch 1:  Loss =  0.245272    Accuracy =  0.5248\n",
      "Epoch 130, CIFAR-10 Batch 1:  Loss =  0.246676    Accuracy =  0.5334\n",
      "Epoch 131, CIFAR-10 Batch 1:  Loss =  0.239286    Accuracy =  0.522\n",
      "Epoch 132, CIFAR-10 Batch 1:  Loss =  0.237647    Accuracy =  0.5242\n",
      "Epoch 133, CIFAR-10 Batch 1:  Loss =  0.225303    Accuracy =  0.5352\n",
      "Epoch 134, CIFAR-10 Batch 1:  Loss =  0.256348    Accuracy =  0.5312\n",
      "Epoch 135, CIFAR-10 Batch 1:  Loss =  0.248664    Accuracy =  0.523\n",
      "Epoch 136, CIFAR-10 Batch 1:  Loss =  0.223474    Accuracy =  0.5266\n",
      "Epoch 137, CIFAR-10 Batch 1:  Loss =  0.228731    Accuracy =  0.532\n",
      "Epoch 138, CIFAR-10 Batch 1:  Loss =  0.22114    Accuracy =  0.5244\n",
      "Epoch 139, CIFAR-10 Batch 1:  Loss =  0.224958    Accuracy =  0.516\n",
      "Epoch 140, CIFAR-10 Batch 1:  Loss =  0.230725    Accuracy =  0.5168\n",
      "Epoch 141, CIFAR-10 Batch 1:  Loss =  0.210637    Accuracy =  0.5246\n",
      "Epoch 142, CIFAR-10 Batch 1:  Loss =  0.204525    Accuracy =  0.5246\n",
      "Epoch 143, CIFAR-10 Batch 1:  Loss =  0.19935    Accuracy =  0.5264\n",
      "Epoch 144, CIFAR-10 Batch 1:  Loss =  0.204465    Accuracy =  0.5272\n",
      "Epoch 145, CIFAR-10 Batch 1:  Loss =  0.218712    Accuracy =  0.5252\n",
      "Epoch 146, CIFAR-10 Batch 1:  Loss =  0.202137    Accuracy =  0.5296\n",
      "Epoch 147, CIFAR-10 Batch 1:  Loss =  0.191868    Accuracy =  0.5206\n",
      "Epoch 148, CIFAR-10 Batch 1:  Loss =  0.193439    Accuracy =  0.52\n",
      "Epoch 149, CIFAR-10 Batch 1:  Loss =  0.21245    Accuracy =  0.5058\n",
      "Epoch 150, CIFAR-10 Batch 1:  Loss =  0.198046    Accuracy =  0.5258\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss =  2.29814    Accuracy =  0.1068\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss =  2.18085    Accuracy =  0.1696\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss =  2.16339    Accuracy =  0.1816\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss =  2.1089    Accuracy =  0.2158\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss =  2.08242    Accuracy =  0.2308\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss =  2.04395    Accuracy =  0.2442\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss =  1.98983    Accuracy =  0.2544\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss =  1.90335    Accuracy =  0.2568\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss =  1.84306    Accuracy =  0.2656\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss =  1.87579    Accuracy =  0.28\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss =  1.93006    Accuracy =  0.2702\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss =  1.82109    Accuracy =  0.3034\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss =  1.69477    Accuracy =  0.2974\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss =  1.68842    Accuracy =  0.3294\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss =  1.7678    Accuracy =  0.336\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss =  1.80985    Accuracy =  0.327\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss =  1.77115    Accuracy =  0.3386\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss =  1.61168    Accuracy =  0.3342\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss =  1.58571    Accuracy =  0.3532\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss =  1.69303    Accuracy =  0.3396\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss =  1.72728    Accuracy =  0.3508\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss =  1.7229    Accuracy =  0.3624\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss =  1.57285    Accuracy =  0.3506\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss =  1.53614    Accuracy =  0.3742\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss =  1.6241    Accuracy =  0.364\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss =  1.656    Accuracy =  0.388\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss =  1.66033    Accuracy =  0.3792\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss =  1.52846    Accuracy =  0.3834\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss =  1.5004    Accuracy =  0.3886\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss =  1.57051    Accuracy =  0.3858\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss =  1.60462    Accuracy =  0.3894\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss =  1.61714    Accuracy =  0.406\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss =  1.46976    Accuracy =  0.402\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss =  1.44517    Accuracy =  0.4066\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss =  1.53177    Accuracy =  0.3936\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss =  1.56991    Accuracy =  0.408\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss =  1.57692    Accuracy =  0.3992\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss =  1.50101    Accuracy =  0.4118\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss =  1.40214    Accuracy =  0.4236\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss =  1.47541    Accuracy =  0.4276\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss =  1.55018    Accuracy =  0.4244\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss =  1.50458    Accuracy =  0.4306\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss =  1.40248    Accuracy =  0.435\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss =  1.36007    Accuracy =  0.4366\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss =  1.46676    Accuracy =  0.417\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss =  1.51683    Accuracy =  0.4314\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss =  1.49933    Accuracy =  0.4436\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss =  1.36339    Accuracy =  0.4416\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss =  1.34916    Accuracy =  0.4438\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss =  1.41084    Accuracy =  0.4362\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss =  1.47649    Accuracy =  0.4536\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss =  1.45468    Accuracy =  0.4616\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss =  1.2872    Accuracy =  0.4586\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss =  1.28357    Accuracy =  0.4674\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss =  1.36148    Accuracy =  0.4732\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss =  1.44429    Accuracy =  0.4532\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss =  1.39443    Accuracy =  0.4708\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss =  1.20825    Accuracy =  0.4808\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss =  1.25549    Accuracy =  0.4806\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss =  1.31768    Accuracy =  0.4814\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss =  1.34883    Accuracy =  0.484\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss =  1.38397    Accuracy =  0.4758\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss =  1.18024    Accuracy =  0.4882\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss =  1.21536    Accuracy =  0.4894\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss =  1.29894    Accuracy =  0.4822\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss =  1.32468    Accuracy =  0.4946\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss =  1.33878    Accuracy =  0.4996\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss =  1.12476    Accuracy =  0.5022\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss =  1.15524    Accuracy =  0.5098\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss =  1.23508    Accuracy =  0.5018\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss =  1.2293    Accuracy =  0.5254\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss =  1.29906    Accuracy =  0.4978\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss =  1.05156    Accuracy =  0.5192\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss =  1.12028    Accuracy =  0.5282\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss =  1.16763    Accuracy =  0.5292\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss =  1.19798    Accuracy =  0.5232\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss =  1.23674    Accuracy =  0.5288\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss =  1.01906    Accuracy =  0.5314\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss =  1.06078    Accuracy =  0.5454\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss =  1.09139    Accuracy =  0.5436\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss =  1.11036    Accuracy =  0.543\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss =  1.21189    Accuracy =  0.533\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss =  1.044    Accuracy =  0.5322\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss =  0.996791    Accuracy =  0.5482\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss =  1.09892    Accuracy =  0.5416\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss =  1.11208    Accuracy =  0.5532\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss =  1.20188    Accuracy =  0.527\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss =  0.918716    Accuracy =  0.5564\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss =  0.975162    Accuracy =  0.5646\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss =  1.04148    Accuracy =  0.5568\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss =  1.06327    Accuracy =  0.5422\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss =  1.12537    Accuracy =  0.556\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss =  0.943313    Accuracy =  0.5558\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss =  0.963541    Accuracy =  0.5576\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss =  0.994185    Accuracy =  0.568\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss =  1.10093    Accuracy =  0.5506\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss =  1.09424    Accuracy =  0.564\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss =  0.918582    Accuracy =  0.563\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss =  0.927297    Accuracy =  0.5654\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss =  0.977413    Accuracy =  0.5596\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss =  1.03368    Accuracy =  0.5376\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss =  1.06189    Accuracy =  0.566\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss =  0.915062    Accuracy =  0.567\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss =  0.920678    Accuracy =  0.5648\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss =  0.969168    Accuracy =  0.5652\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss =  0.962102    Accuracy =  0.5624\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss =  1.00177    Accuracy =  0.5604\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss =  0.890766    Accuracy =  0.5796\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss =  0.887147    Accuracy =  0.5722\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss =  0.949095    Accuracy =  0.561\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss =  0.912875    Accuracy =  0.567\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss =  1.0028    Accuracy =  0.5726\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss =  0.881031    Accuracy =  0.5718\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss =  0.822933    Accuracy =  0.5832\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss =  0.893333    Accuracy =  0.5732\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss =  0.912329    Accuracy =  0.5696\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss =  0.966863    Accuracy =  0.5776\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss =  0.798727    Accuracy =  0.586\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss =  0.826874    Accuracy =  0.5924\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss =  0.865282    Accuracy =  0.5754\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss =  0.873458    Accuracy =  0.5822\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss =  0.91004    Accuracy =  0.5824\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss =  0.823529    Accuracy =  0.5846\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss =  0.809244    Accuracy =  0.5992\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss =  0.842796    Accuracy =  0.5776\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss =  0.838616    Accuracy =  0.5782\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss =  0.88187    Accuracy =  0.5988\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss =  0.844215    Accuracy =  0.5976\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss =  0.787584    Accuracy =  0.6002\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss =  0.837777    Accuracy =  0.5758\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss =  0.839489    Accuracy =  0.5786\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss =  0.898579    Accuracy =  0.5886\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss =  0.800583    Accuracy =  0.5914\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss =  0.734309    Accuracy =  0.6048\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss =  0.819068    Accuracy =  0.595\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss =  0.773935    Accuracy =  0.584\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss =  0.827759    Accuracy =  0.6084\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss =  0.758788    Accuracy =  0.6044\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss =  0.73943    Accuracy =  0.602\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss =  0.79518    Accuracy =  0.5848\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss =  0.763651    Accuracy =  0.5916\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss =  0.876362    Accuracy =  0.5956\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss =  0.79271    Accuracy =  0.5868\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss =  0.704482    Accuracy =  0.6044\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss =  0.789363    Accuracy =  0.5904\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss =  0.761688    Accuracy =  0.5966\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss =  0.866446    Accuracy =  0.5884\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss =  0.717417    Accuracy =  0.6048\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss =  0.686323    Accuracy =  0.61\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss =  0.710381    Accuracy =  0.597\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss =  0.737967    Accuracy =  0.5996\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss =  0.795445    Accuracy =  0.6114\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss =  0.716132    Accuracy =  0.6134\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss =  0.691522    Accuracy =  0.6086\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss =  0.719725    Accuracy =  0.6036\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss =  0.730658    Accuracy =  0.6182\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss =  0.745722    Accuracy =  0.6104\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss =  0.67061    Accuracy =  0.5948\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss =  0.634525    Accuracy =  0.6126\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss =  0.662548    Accuracy =  0.6102\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss =  0.681289    Accuracy =  0.6158\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss =  0.759376    Accuracy =  0.6074\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss =  0.654982    Accuracy =  0.6142\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss =  0.616188    Accuracy =  0.622\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss =  0.681301    Accuracy =  0.609\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss =  0.664285    Accuracy =  0.6118\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss =  0.708897    Accuracy =  0.6154\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss =  0.63461    Accuracy =  0.6192\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss =  0.622318    Accuracy =  0.622\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss =  0.658751    Accuracy =  0.6238\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss =  0.61803    Accuracy =  0.6274\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss =  0.692711    Accuracy =  0.6244\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss =  0.625968    Accuracy =  0.6166\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss =  0.575246    Accuracy =  0.6166\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss =  0.63561    Accuracy =  0.6126\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss =  0.605745    Accuracy =  0.6292\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss =  0.698765    Accuracy =  0.6212\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss =  0.609307    Accuracy =  0.626\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss =  0.585016    Accuracy =  0.6322\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss =  0.6229    Accuracy =  0.6138\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss =  0.568674    Accuracy =  0.6326\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss =  0.673568    Accuracy =  0.6242\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss =  0.562478    Accuracy =  0.6282\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss =  0.564949    Accuracy =  0.629\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss =  0.584443    Accuracy =  0.6296\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss =  0.61682    Accuracy =  0.6132\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss =  0.626889    Accuracy =  0.625\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss =  0.571939    Accuracy =  0.6238\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss =  0.529874    Accuracy =  0.6372\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss =  0.571823    Accuracy =  0.6136\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss =  0.60218    Accuracy =  0.6112\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss =  0.636461    Accuracy =  0.6196\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss =  0.580528    Accuracy =  0.6202\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss =  0.544167    Accuracy =  0.6294\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss =  0.573733    Accuracy =  0.622\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss =  0.550797    Accuracy =  0.6244\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss =  0.63725    Accuracy =  0.6268\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss =  0.561904    Accuracy =  0.6192\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss =  0.544188    Accuracy =  0.6268\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss =  0.562503    Accuracy =  0.6234\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss =  0.540936    Accuracy =  0.6276\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss =  0.599248    Accuracy =  0.6258\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss =  0.503366    Accuracy =  0.636\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss =  0.493553    Accuracy =  0.6344\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss =  0.536206    Accuracy =  0.6314\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss =  0.51078    Accuracy =  0.6308\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss =  0.613651    Accuracy =  0.6306\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss =  0.529164    Accuracy =  0.6304\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss =  0.503737    Accuracy =  0.6308\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss =  0.570013    Accuracy =  0.6102\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss =  0.526903    Accuracy =  0.6314\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss =  0.584012    Accuracy =  0.631\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss =  0.531962    Accuracy =  0.6346\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss =  0.514134    Accuracy =  0.617\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss =  0.530523    Accuracy =  0.6288\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss =  0.5253    Accuracy =  0.646\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss =  0.547876    Accuracy =  0.6322\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss =  0.508689    Accuracy =  0.6246\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss =  0.462642    Accuracy =  0.6344\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss =  0.509947    Accuracy =  0.6306\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss =  0.485124    Accuracy =  0.6392\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss =  0.556184    Accuracy =  0.6358\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss =  0.506259    Accuracy =  0.6308\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss =  0.447681    Accuracy =  0.6432\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss =  0.48627    Accuracy =  0.6378\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss =  0.473777    Accuracy =  0.6498\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss =  0.52869    Accuracy =  0.6354\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss =  0.473974    Accuracy =  0.635\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss =  0.449538    Accuracy =  0.6304\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss =  0.502909    Accuracy =  0.6188\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss =  0.475273    Accuracy =  0.6438\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss =  0.531091    Accuracy =  0.6286\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss =  0.472161    Accuracy =  0.6358\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss =  0.474757    Accuracy =  0.6326\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss =  0.497098    Accuracy =  0.6238\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss =  0.443377    Accuracy =  0.6348\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss =  0.536926    Accuracy =  0.6306\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss =  0.484208    Accuracy =  0.6412\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss =  0.426596    Accuracy =  0.6416\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss =  0.501845    Accuracy =  0.6172\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss =  0.495236    Accuracy =  0.642\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss =  0.566445    Accuracy =  0.6194\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss =  0.459434    Accuracy =  0.6424\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss =  0.434624    Accuracy =  0.6346\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss =  0.470144    Accuracy =  0.6402\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss =  0.480223    Accuracy =  0.643\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss =  0.502915    Accuracy =  0.6232\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss =  0.439022    Accuracy =  0.6416\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss =  0.397452    Accuracy =  0.639\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss =  0.449939    Accuracy =  0.6348\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss =  0.424766    Accuracy =  0.6328\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss =  0.519935    Accuracy =  0.6158\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss =  0.500409    Accuracy =  0.6224\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss =  0.43077    Accuracy =  0.631\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss =  0.476424    Accuracy =  0.6374\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss =  0.429168    Accuracy =  0.6444\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss =  0.478694    Accuracy =  0.6448\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss =  0.440201    Accuracy =  0.6438\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss =  0.402899    Accuracy =  0.6344\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss =  0.474983    Accuracy =  0.6406\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss =  0.435131    Accuracy =  0.6502\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss =  0.460198    Accuracy =  0.6422\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss =  0.429596    Accuracy =  0.643\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss =  0.389333    Accuracy =  0.6394\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss =  0.451732    Accuracy =  0.6326\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss =  0.415065    Accuracy =  0.6456\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss =  0.469931    Accuracy =  0.629\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss =  0.405529    Accuracy =  0.6444\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss =  0.393405    Accuracy =  0.6404\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss =  0.419599    Accuracy =  0.6338\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss =  0.418865    Accuracy =  0.6492\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss =  0.429746    Accuracy =  0.6392\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss =  0.414464    Accuracy =  0.6462\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss =  0.356915    Accuracy =  0.6452\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss =  0.421019    Accuracy =  0.647\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss =  0.430075    Accuracy =  0.6402\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss =  0.462988    Accuracy =  0.6364\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss =  0.414517    Accuracy =  0.6398\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss =  0.391271    Accuracy =  0.64\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss =  0.424532    Accuracy =  0.6238\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss =  0.379064    Accuracy =  0.6474\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss =  0.461872    Accuracy =  0.624\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss =  0.413458    Accuracy =  0.6382\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss =  0.368651    Accuracy =  0.6488\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss =  0.392857    Accuracy =  0.6354\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss =  0.37786    Accuracy =  0.6484\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss =  0.459327    Accuracy =  0.6246\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss =  0.392348    Accuracy =  0.6424\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss =  0.354987    Accuracy =  0.6372\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss =  0.40191    Accuracy =  0.647\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss =  0.377266    Accuracy =  0.644\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss =  0.431708    Accuracy =  0.635\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss =  0.401425    Accuracy =  0.6366\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss =  0.37975    Accuracy =  0.6286\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss =  0.408986    Accuracy =  0.642\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss =  0.373759    Accuracy =  0.6466\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss =  0.422588    Accuracy =  0.6364\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss =  0.412693    Accuracy =  0.626\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss =  0.359537    Accuracy =  0.6458\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss =  0.376546    Accuracy =  0.6424\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss =  0.350639    Accuracy =  0.6496\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss =  0.453036    Accuracy =  0.6324\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss =  0.340535    Accuracy =  0.641\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss =  0.335681    Accuracy =  0.6514\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss =  0.385162    Accuracy =  0.6354\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss =  0.372863    Accuracy =  0.6482\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss =  0.41122    Accuracy =  0.628\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss =  0.37688    Accuracy =  0.6396\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss =  0.363585    Accuracy =  0.6394\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss =  0.347856    Accuracy =  0.6416\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss =  0.373217    Accuracy =  0.6456\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss =  0.40121    Accuracy =  0.631\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss =  0.366101    Accuracy =  0.6404\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss =  0.329687    Accuracy =  0.6426\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss =  0.355922    Accuracy =  0.6434\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss =  0.379295    Accuracy =  0.6344\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss =  0.399852    Accuracy =  0.6342\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss =  0.36546    Accuracy =  0.6372\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss =  0.335024    Accuracy =  0.6358\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss =  0.356024    Accuracy =  0.6378\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss =  0.362902    Accuracy =  0.646\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss =  0.398806    Accuracy =  0.6338\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss =  0.32379    Accuracy =  0.6396\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss =  0.299438    Accuracy =  0.6524\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss =  0.337577    Accuracy =  0.648\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss =  0.346331    Accuracy =  0.6508\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss =  0.353445    Accuracy =  0.6398\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss =  0.3471    Accuracy =  0.6286\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss =  0.327867    Accuracy =  0.6354\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss =  0.347156    Accuracy =  0.6444\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss =  0.352609    Accuracy =  0.6482\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss =  0.368841    Accuracy =  0.6476\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss =  0.351601    Accuracy =  0.6268\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss =  0.303728    Accuracy =  0.64\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss =  0.351485    Accuracy =  0.6422\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss =  0.325959    Accuracy =  0.6516\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss =  0.354849    Accuracy =  0.6406\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss =  0.371261    Accuracy =  0.6294\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss =  0.352832    Accuracy =  0.6318\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss =  0.342516    Accuracy =  0.6378\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss =  0.323281    Accuracy =  0.6416\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss =  0.359468    Accuracy =  0.6432\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss =  0.301617    Accuracy =  0.6482\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss =  0.287952    Accuracy =  0.6488\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss =  0.317478    Accuracy =  0.641\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss =  0.332393    Accuracy =  0.644\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss =  0.376518    Accuracy =  0.6298\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss =  0.30749    Accuracy =  0.6304\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss =  0.303479    Accuracy =  0.6382\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss =  0.350464    Accuracy =  0.6328\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss =  0.300097    Accuracy =  0.6448\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss =  0.314242    Accuracy =  0.6452\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss =  0.289076    Accuracy =  0.6388\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss =  0.305507    Accuracy =  0.6384\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss =  0.328112    Accuracy =  0.639\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss =  0.327228    Accuracy =  0.6462\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss =  0.323403    Accuracy =  0.6412\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss =  0.303021    Accuracy =  0.6362\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss =  0.284988    Accuracy =  0.6388\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss =  0.3063    Accuracy =  0.6448\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss =  0.310448    Accuracy =  0.6376\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss =  0.326219    Accuracy =  0.6438\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss =  0.307346    Accuracy =  0.6362\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss =  0.283358    Accuracy =  0.6486\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss =  0.308531    Accuracy =  0.633\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss =  0.309108    Accuracy =  0.6508\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss =  0.316039    Accuracy =  0.6396\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss =  0.28672    Accuracy =  0.6446\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss =  0.303739    Accuracy =  0.6426\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss =  0.316823    Accuracy =  0.6444\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss =  0.292373    Accuracy =  0.6496\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss =  0.292411    Accuracy =  0.6506\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss =  0.302057    Accuracy =  0.6416\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss =  0.296515    Accuracy =  0.6438\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss =  0.288097    Accuracy =  0.6414\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss =  0.298104    Accuracy =  0.6448\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss =  0.288295    Accuracy =  0.6464\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss =  0.271687    Accuracy =  0.6512\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss =  0.273325    Accuracy =  0.6454\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss =  0.314396    Accuracy =  0.6402\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss =  0.295531    Accuracy =  0.642\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss =  0.285904    Accuracy =  0.6434\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss =  0.266201    Accuracy =  0.6508\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss =  0.273891    Accuracy =  0.6394\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss =  0.280224    Accuracy =  0.6394\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss =  0.26784    Accuracy =  0.6488\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss =  0.282713    Accuracy =  0.6342\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss =  0.270157    Accuracy =  0.6416\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss =  0.258758    Accuracy =  0.6412\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss =  0.290598    Accuracy =  0.6376\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss =  0.275352    Accuracy =  0.64\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss =  0.296964    Accuracy =  0.6382\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss =  0.248851    Accuracy =  0.6466\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss =  0.262899    Accuracy =  0.65\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss =  0.293184    Accuracy =  0.641\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss =  0.261076    Accuracy =  0.651\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss =  0.299909    Accuracy =  0.6394\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss =  0.264703    Accuracy =  0.65\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss =  0.235648    Accuracy =  0.6576\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss =  0.269933    Accuracy =  0.6384\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss =  0.286421    Accuracy =  0.6396\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss =  0.28046    Accuracy =  0.6314\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss =  0.268674    Accuracy =  0.6424\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss =  0.276369    Accuracy =  0.634\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss =  0.268803    Accuracy =  0.6436\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss =  0.277791    Accuracy =  0.6344\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss =  0.277767    Accuracy =  0.6368\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss =  0.281542    Accuracy =  0.6352\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss =  0.258584    Accuracy =  0.6422\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss =  0.280087    Accuracy =  0.648\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss =  0.249454    Accuracy =  0.6442\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss =  0.274854    Accuracy =  0.6386\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss =  0.250275    Accuracy =  0.6398\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss =  0.259861    Accuracy =  0.6474\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss =  0.262402    Accuracy =  0.6426\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss =  0.257482    Accuracy =  0.633\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss =  0.27937    Accuracy =  0.6458\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss =  0.232219    Accuracy =  0.6468\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss =  0.214854    Accuracy =  0.652\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss =  0.274018    Accuracy =  0.643\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss =  0.248136    Accuracy =  0.6326\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss =  0.313616    Accuracy =  0.6262\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss =  0.268412    Accuracy =  0.6374\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss =  0.242052    Accuracy =  0.6492\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss =  0.262729    Accuracy =  0.6484\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss =  0.236934    Accuracy =  0.64\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss =  0.257732    Accuracy =  0.642\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss =  0.219275    Accuracy =  0.6428\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss =  0.221078    Accuracy =  0.647\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss =  0.279338    Accuracy =  0.6454\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss =  0.240661    Accuracy =  0.6478\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss =  0.250792    Accuracy =  0.6472\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss =  0.227045    Accuracy =  0.6426\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss =  0.252    Accuracy =  0.6486\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss =  0.261478    Accuracy =  0.652\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss =  0.239498    Accuracy =  0.6454\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss =  0.252974    Accuracy =  0.6424\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss =  0.20659    Accuracy =  0.6444\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss =  0.209926    Accuracy =  0.6526\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss =  0.231068    Accuracy =  0.653\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss =  0.220447    Accuracy =  0.6386\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss =  0.255    Accuracy =  0.6404\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss =  0.219372    Accuracy =  0.643\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss =  0.20597    Accuracy =  0.6468\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss =  0.23684    Accuracy =  0.6458\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss =  0.243038    Accuracy =  0.6392\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss =  0.244223    Accuracy =  0.6428\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss =  0.201044    Accuracy =  0.6488\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss =  0.208758    Accuracy =  0.6476\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss =  0.231021    Accuracy =  0.648\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss =  0.247801    Accuracy =  0.642\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss =  0.24931    Accuracy =  0.6398\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss =  0.214296    Accuracy =  0.6444\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss =  0.213425    Accuracy =  0.642\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss =  0.274597    Accuracy =  0.643\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss =  0.212405    Accuracy =  0.6386\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss =  0.231838    Accuracy =  0.645\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss =  0.218072    Accuracy =  0.6416\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss =  0.202354    Accuracy =  0.6422\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss =  0.257095    Accuracy =  0.649\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss =  0.218662    Accuracy =  0.6356\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss =  0.226641    Accuracy =  0.6434\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss =  0.193595    Accuracy =  0.6478\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss =  0.204373    Accuracy =  0.6406\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss =  0.225504    Accuracy =  0.6494\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss =  0.222202    Accuracy =  0.6392\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss =  0.217517    Accuracy =  0.642\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss =  0.212874    Accuracy =  0.6366\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss =  0.211707    Accuracy =  0.6414\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss =  0.216443    Accuracy =  0.6516\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss =  0.223133    Accuracy =  0.6324\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss =  0.220349    Accuracy =  0.6414\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss =  0.207525    Accuracy =  0.6292\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss =  0.190052    Accuracy =  0.6514\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss =  0.236962    Accuracy =  0.6474\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss =  0.22678    Accuracy =  0.6412\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss =  0.249583    Accuracy =  0.631\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss =  0.208042    Accuracy =  0.6386\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss =  0.212728    Accuracy =  0.6372\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss =  0.225677    Accuracy =  0.652\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss =  0.219407    Accuracy =  0.6346\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss =  0.217603    Accuracy =  0.6426\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss =  0.203307    Accuracy =  0.6444\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss =  0.188215    Accuracy =  0.6474\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss =  0.206036    Accuracy =  0.6476\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss =  0.197127    Accuracy =  0.6314\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss =  0.247694    Accuracy =  0.6284\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss =  0.196316    Accuracy =  0.6446\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss =  0.198135    Accuracy =  0.6392\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss =  0.229532    Accuracy =  0.6408\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss =  0.210196    Accuracy =  0.6334\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss =  0.206626    Accuracy =  0.6466\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss =  0.199641    Accuracy =  0.6418\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss =  0.19669    Accuracy =  0.6444\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss =  0.255183    Accuracy =  0.6398\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss =  0.208534    Accuracy =  0.6416\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss =  0.221087    Accuracy =  0.644\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss =  0.185447    Accuracy =  0.6504\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss =  0.194623    Accuracy =  0.6434\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss =  0.191727    Accuracy =  0.6484\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss =  0.218562    Accuracy =  0.6308\n",
      "Epoch 101, CIFAR-10 Batch 2:  Loss =  0.221403    Accuracy =  0.6508\n",
      "Epoch 101, CIFAR-10 Batch 3:  Loss =  0.192432    Accuracy =  0.65\n",
      "Epoch 101, CIFAR-10 Batch 4:  Loss =  0.187959    Accuracy =  0.645\n",
      "Epoch 101, CIFAR-10 Batch 5:  Loss =  0.187897    Accuracy =  0.6488\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss =  0.196351    Accuracy =  0.6468\n",
      "Epoch 102, CIFAR-10 Batch 2:  Loss =  0.23487    Accuracy =  0.6344\n",
      "Epoch 102, CIFAR-10 Batch 3:  Loss =  0.219938    Accuracy =  0.6408\n",
      "Epoch 102, CIFAR-10 Batch 4:  Loss =  0.187033    Accuracy =  0.6468\n",
      "Epoch 102, CIFAR-10 Batch 5:  Loss =  0.190864    Accuracy =  0.6478\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss =  0.189333    Accuracy =  0.6408\n",
      "Epoch 103, CIFAR-10 Batch 2:  Loss =  0.198122    Accuracy =  0.6488\n",
      "Epoch 103, CIFAR-10 Batch 3:  Loss =  0.188205    Accuracy =  0.6394\n",
      "Epoch 103, CIFAR-10 Batch 4:  Loss =  0.184643    Accuracy =  0.6434\n",
      "Epoch 103, CIFAR-10 Batch 5:  Loss =  0.194086    Accuracy =  0.6444\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss =  0.20297    Accuracy =  0.6332\n",
      "Epoch 104, CIFAR-10 Batch 2:  Loss =  0.218911    Accuracy =  0.6426\n",
      "Epoch 104, CIFAR-10 Batch 3:  Loss =  0.191542    Accuracy =  0.6372\n",
      "Epoch 104, CIFAR-10 Batch 4:  Loss =  0.188552    Accuracy =  0.6476\n",
      "Epoch 104, CIFAR-10 Batch 5:  Loss =  0.189024    Accuracy =  0.6398\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss =  0.198535    Accuracy =  0.6444\n",
      "Epoch 105, CIFAR-10 Batch 2:  Loss =  0.21512    Accuracy =  0.6364\n",
      "Epoch 105, CIFAR-10 Batch 3:  Loss =  0.182994    Accuracy =  0.651\n",
      "Epoch 105, CIFAR-10 Batch 4:  Loss =  0.191855    Accuracy =  0.6454\n",
      "Epoch 105, CIFAR-10 Batch 5:  Loss =  0.183473    Accuracy =  0.65\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss =  0.176294    Accuracy =  0.6408\n",
      "Epoch 106, CIFAR-10 Batch 2:  Loss =  0.19344    Accuracy =  0.6418\n",
      "Epoch 106, CIFAR-10 Batch 3:  Loss =  0.183266    Accuracy =  0.6388\n",
      "Epoch 106, CIFAR-10 Batch 4:  Loss =  0.192116    Accuracy =  0.648\n",
      "Epoch 106, CIFAR-10 Batch 5:  Loss =  0.181778    Accuracy =  0.6464\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss =  0.208989    Accuracy =  0.6454\n",
      "Epoch 107, CIFAR-10 Batch 2:  Loss =  0.193099    Accuracy =  0.6478\n",
      "Epoch 107, CIFAR-10 Batch 3:  Loss =  0.194373    Accuracy =  0.6412\n",
      "Epoch 107, CIFAR-10 Batch 4:  Loss =  0.183896    Accuracy =  0.6462\n",
      "Epoch 107, CIFAR-10 Batch 5:  Loss =  0.182515    Accuracy =  0.6572\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss =  0.231915    Accuracy =  0.6424\n",
      "Epoch 108, CIFAR-10 Batch 2:  Loss =  0.189049    Accuracy =  0.6462\n",
      "Epoch 108, CIFAR-10 Batch 3:  Loss =  0.162512    Accuracy =  0.6426\n",
      "Epoch 108, CIFAR-10 Batch 4:  Loss =  0.175778    Accuracy =  0.6502\n",
      "Epoch 108, CIFAR-10 Batch 5:  Loss =  0.185908    Accuracy =  0.646\n",
      "Epoch 109, CIFAR-10 Batch 1:  Loss =  0.174748    Accuracy =  0.6398\n",
      "Epoch 109, CIFAR-10 Batch 2:  Loss =  0.19465    Accuracy =  0.6426\n",
      "Epoch 109, CIFAR-10 Batch 3:  Loss =  0.162873    Accuracy =  0.642\n",
      "Epoch 109, CIFAR-10 Batch 4:  Loss =  0.192089    Accuracy =  0.6354\n",
      "Epoch 109, CIFAR-10 Batch 5:  Loss =  0.176402    Accuracy =  0.6492\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss =  0.182361    Accuracy =  0.645\n",
      "Epoch 110, CIFAR-10 Batch 2:  Loss =  0.192183    Accuracy =  0.643\n",
      "Epoch 110, CIFAR-10 Batch 3:  Loss =  0.176353    Accuracy =  0.6408\n",
      "Epoch 110, CIFAR-10 Batch 4:  Loss =  0.177947    Accuracy =  0.6418\n",
      "Epoch 110, CIFAR-10 Batch 5:  Loss =  0.181792    Accuracy =  0.6452\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss =  0.204088    Accuracy =  0.633\n",
      "Epoch 111, CIFAR-10 Batch 2:  Loss =  0.180805    Accuracy =  0.6326\n",
      "Epoch 111, CIFAR-10 Batch 3:  Loss =  0.16224    Accuracy =  0.6324\n",
      "Epoch 111, CIFAR-10 Batch 4:  Loss =  0.179599    Accuracy =  0.6424\n",
      "Epoch 111, CIFAR-10 Batch 5:  Loss =  0.182609    Accuracy =  0.6462\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss =  0.181775    Accuracy =  0.6444\n",
      "Epoch 112, CIFAR-10 Batch 2:  Loss =  0.177696    Accuracy =  0.6334\n",
      "Epoch 112, CIFAR-10 Batch 3:  Loss =  0.174093    Accuracy =  0.6464\n",
      "Epoch 112, CIFAR-10 Batch 4:  Loss =  0.169    Accuracy =  0.6464\n",
      "Epoch 112, CIFAR-10 Batch 5:  Loss =  0.182424    Accuracy =  0.6512\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss =  0.186383    Accuracy =  0.6468\n",
      "Epoch 113, CIFAR-10 Batch 2:  Loss =  0.1861    Accuracy =  0.64\n",
      "Epoch 113, CIFAR-10 Batch 3:  Loss =  0.171968    Accuracy =  0.6432\n",
      "Epoch 113, CIFAR-10 Batch 4:  Loss =  0.163217    Accuracy =  0.6506\n",
      "Epoch 113, CIFAR-10 Batch 5:  Loss =  0.17073    Accuracy =  0.651\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss =  0.178717    Accuracy =  0.6376\n",
      "Epoch 114, CIFAR-10 Batch 2:  Loss =  0.175238    Accuracy =  0.6474\n",
      "Epoch 114, CIFAR-10 Batch 3:  Loss =  0.161211    Accuracy =  0.6414\n",
      "Epoch 114, CIFAR-10 Batch 4:  Loss =  0.164758    Accuracy =  0.639\n",
      "Epoch 114, CIFAR-10 Batch 5:  Loss =  0.182262    Accuracy =  0.6436\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss =  0.190125    Accuracy =  0.639\n",
      "Epoch 115, CIFAR-10 Batch 2:  Loss =  0.177148    Accuracy =  0.6458\n",
      "Epoch 115, CIFAR-10 Batch 3:  Loss =  0.170673    Accuracy =  0.637\n",
      "Epoch 115, CIFAR-10 Batch 4:  Loss =  0.155606    Accuracy =  0.6406\n",
      "Epoch 115, CIFAR-10 Batch 5:  Loss =  0.165157    Accuracy =  0.6508\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss =  0.194094    Accuracy =  0.6416\n",
      "Epoch 116, CIFAR-10 Batch 2:  Loss =  0.198074    Accuracy =  0.648\n",
      "Epoch 116, CIFAR-10 Batch 3:  Loss =  0.153865    Accuracy =  0.6428\n",
      "Epoch 116, CIFAR-10 Batch 4:  Loss =  0.184269    Accuracy =  0.6408\n",
      "Epoch 116, CIFAR-10 Batch 5:  Loss =  0.163436    Accuracy =  0.65\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss =  0.168623    Accuracy =  0.64\n",
      "Epoch 117, CIFAR-10 Batch 2:  Loss =  0.185601    Accuracy =  0.638\n",
      "Epoch 117, CIFAR-10 Batch 3:  Loss =  0.165734    Accuracy =  0.6496\n",
      "Epoch 117, CIFAR-10 Batch 4:  Loss =  0.164364    Accuracy =  0.6432\n",
      "Epoch 117, CIFAR-10 Batch 5:  Loss =  0.164522    Accuracy =  0.6482\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss =  0.172452    Accuracy =  0.6524\n",
      "Epoch 118, CIFAR-10 Batch 2:  Loss =  0.183127    Accuracy =  0.64\n",
      "Epoch 118, CIFAR-10 Batch 3:  Loss =  0.159135    Accuracy =  0.639\n",
      "Epoch 118, CIFAR-10 Batch 4:  Loss =  0.163787    Accuracy =  0.6356\n",
      "Epoch 118, CIFAR-10 Batch 5:  Loss =  0.147227    Accuracy =  0.6412\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss =  0.173619    Accuracy =  0.6442\n",
      "Epoch 119, CIFAR-10 Batch 2:  Loss =  0.189395    Accuracy =  0.6386\n",
      "Epoch 119, CIFAR-10 Batch 3:  Loss =  0.143398    Accuracy =  0.6416\n",
      "Epoch 119, CIFAR-10 Batch 4:  Loss =  0.165875    Accuracy =  0.637\n",
      "Epoch 119, CIFAR-10 Batch 5:  Loss =  0.158091    Accuracy =  0.648\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss =  0.19481    Accuracy =  0.6412\n",
      "Epoch 120, CIFAR-10 Batch 2:  Loss =  0.185346    Accuracy =  0.6414\n",
      "Epoch 120, CIFAR-10 Batch 3:  Loss =  0.156261    Accuracy =  0.629\n",
      "Epoch 120, CIFAR-10 Batch 4:  Loss =  0.18244    Accuracy =  0.634\n",
      "Epoch 120, CIFAR-10 Batch 5:  Loss =  0.166046    Accuracy =  0.639\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss =  0.169374    Accuracy =  0.639\n",
      "Epoch 121, CIFAR-10 Batch 2:  Loss =  0.183321    Accuracy =  0.6344\n",
      "Epoch 121, CIFAR-10 Batch 3:  Loss =  0.149915    Accuracy =  0.6438\n",
      "Epoch 121, CIFAR-10 Batch 4:  Loss =  0.171097    Accuracy =  0.634\n",
      "Epoch 121, CIFAR-10 Batch 5:  Loss =  0.16618    Accuracy =  0.6336\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss =  0.187412    Accuracy =  0.6396\n",
      "Epoch 122, CIFAR-10 Batch 2:  Loss =  0.161151    Accuracy =  0.6464\n",
      "Epoch 122, CIFAR-10 Batch 3:  Loss =  0.133383    Accuracy =  0.645\n",
      "Epoch 122, CIFAR-10 Batch 4:  Loss =  0.169886    Accuracy =  0.6382\n",
      "Epoch 122, CIFAR-10 Batch 5:  Loss =  0.173352    Accuracy =  0.6372\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss =  0.17593    Accuracy =  0.6458\n",
      "Epoch 123, CIFAR-10 Batch 2:  Loss =  0.151236    Accuracy =  0.6424\n",
      "Epoch 123, CIFAR-10 Batch 3:  Loss =  0.14472    Accuracy =  0.6374\n",
      "Epoch 123, CIFAR-10 Batch 4:  Loss =  0.150146    Accuracy =  0.644\n",
      "Epoch 123, CIFAR-10 Batch 5:  Loss =  0.14414    Accuracy =  0.6486\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss =  0.166813    Accuracy =  0.6418\n",
      "Epoch 124, CIFAR-10 Batch 2:  Loss =  0.163029    Accuracy =  0.646\n",
      "Epoch 124, CIFAR-10 Batch 3:  Loss =  0.139115    Accuracy =  0.6448\n",
      "Epoch 124, CIFAR-10 Batch 4:  Loss =  0.158368    Accuracy =  0.635\n",
      "Epoch 124, CIFAR-10 Batch 5:  Loss =  0.17395    Accuracy =  0.6312\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss =  0.162662    Accuracy =  0.6464\n",
      "Epoch 125, CIFAR-10 Batch 2:  Loss =  0.164333    Accuracy =  0.6494\n",
      "Epoch 125, CIFAR-10 Batch 3:  Loss =  0.131647    Accuracy =  0.642\n",
      "Epoch 125, CIFAR-10 Batch 4:  Loss =  0.13825    Accuracy =  0.6466\n",
      "Epoch 125, CIFAR-10 Batch 5:  Loss =  0.15266    Accuracy =  0.6444\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss =  0.162963    Accuracy =  0.6474\n",
      "Epoch 126, CIFAR-10 Batch 2:  Loss =  0.15746    Accuracy =  0.6466\n",
      "Epoch 126, CIFAR-10 Batch 3:  Loss =  0.121971    Accuracy =  0.6518\n",
      "Epoch 126, CIFAR-10 Batch 4:  Loss =  0.137069    Accuracy =  0.6446\n",
      "Epoch 126, CIFAR-10 Batch 5:  Loss =  0.148723    Accuracy =  0.6458\n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss =  0.173693    Accuracy =  0.6404\n",
      "Epoch 127, CIFAR-10 Batch 2:  Loss =  0.177282    Accuracy =  0.643\n",
      "Epoch 127, CIFAR-10 Batch 3:  Loss =  0.139683    Accuracy =  0.6426\n",
      "Epoch 127, CIFAR-10 Batch 4:  Loss =  0.135684    Accuracy =  0.635\n",
      "Epoch 127, CIFAR-10 Batch 5:  Loss =  0.15564    Accuracy =  0.6448\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss =  0.165438    Accuracy =  0.6462\n",
      "Epoch 128, CIFAR-10 Batch 2:  Loss =  0.173335    Accuracy =  0.6358\n",
      "Epoch 128, CIFAR-10 Batch 3:  Loss =  0.11959    Accuracy =  0.6454\n",
      "Epoch 128, CIFAR-10 Batch 4:  Loss =  0.122417    Accuracy =  0.6446\n",
      "Epoch 128, CIFAR-10 Batch 5:  Loss =  0.134698    Accuracy =  0.6414\n",
      "Epoch 129, CIFAR-10 Batch 1:  Loss =  0.155938    Accuracy =  0.6468\n",
      "Epoch 129, CIFAR-10 Batch 2:  Loss =  0.15907    Accuracy =  0.637\n",
      "Epoch 129, CIFAR-10 Batch 3:  Loss =  0.118729    Accuracy =  0.6464\n",
      "Epoch 129, CIFAR-10 Batch 4:  Loss =  0.143723    Accuracy =  0.638\n",
      "Epoch 129, CIFAR-10 Batch 5:  Loss =  0.152344    Accuracy =  0.6408\n",
      "Epoch 130, CIFAR-10 Batch 1:  Loss =  0.155326    Accuracy =  0.6496\n",
      "Epoch 130, CIFAR-10 Batch 2:  Loss =  0.149649    Accuracy =  0.6416\n",
      "Epoch 130, CIFAR-10 Batch 3:  Loss =  0.131676    Accuracy =  0.6414\n",
      "Epoch 130, CIFAR-10 Batch 4:  Loss =  0.123432    Accuracy =  0.646\n",
      "Epoch 130, CIFAR-10 Batch 5:  Loss =  0.148844    Accuracy =  0.6462\n",
      "Epoch 131, CIFAR-10 Batch 1:  Loss =  0.159521    Accuracy =  0.6458\n",
      "Epoch 131, CIFAR-10 Batch 2:  Loss =  0.162516    Accuracy =  0.6408\n",
      "Epoch 131, CIFAR-10 Batch 3:  Loss =  0.146897    Accuracy =  0.637\n",
      "Epoch 131, CIFAR-10 Batch 4:  Loss =  0.138993    Accuracy =  0.6454\n",
      "Epoch 131, CIFAR-10 Batch 5:  Loss =  0.139926    Accuracy =  0.6456\n",
      "Epoch 132, CIFAR-10 Batch 1:  Loss =  0.149982    Accuracy =  0.651\n",
      "Epoch 132, CIFAR-10 Batch 2:  Loss =  0.137221    Accuracy =  0.6494\n",
      "Epoch 132, CIFAR-10 Batch 3:  Loss =  0.122368    Accuracy =  0.6528\n",
      "Epoch 132, CIFAR-10 Batch 4:  Loss =  0.133566    Accuracy =  0.6446\n",
      "Epoch 132, CIFAR-10 Batch 5:  Loss =  0.148279    Accuracy =  0.6368\n",
      "Epoch 133, CIFAR-10 Batch 1:  Loss =  0.148139    Accuracy =  0.645\n",
      "Epoch 133, CIFAR-10 Batch 2:  Loss =  0.13409    Accuracy =  0.644\n",
      "Epoch 133, CIFAR-10 Batch 3:  Loss =  0.12096    Accuracy =  0.6472\n",
      "Epoch 133, CIFAR-10 Batch 4:  Loss =  0.120555    Accuracy =  0.6468\n",
      "Epoch 133, CIFAR-10 Batch 5:  Loss =  0.12842    Accuracy =  0.6344\n",
      "Epoch 134, CIFAR-10 Batch 1:  Loss =  0.149393    Accuracy =  0.6474\n",
      "Epoch 134, CIFAR-10 Batch 2:  Loss =  0.146819    Accuracy =  0.6386\n",
      "Epoch 134, CIFAR-10 Batch 3:  Loss =  0.123423    Accuracy =  0.63\n",
      "Epoch 134, CIFAR-10 Batch 4:  Loss =  0.125119    Accuracy =  0.638\n",
      "Epoch 134, CIFAR-10 Batch 5:  Loss =  0.12143    Accuracy =  0.6408\n",
      "Epoch 135, CIFAR-10 Batch 1:  Loss =  0.149843    Accuracy =  0.6484\n",
      "Epoch 135, CIFAR-10 Batch 2:  Loss =  0.138812    Accuracy =  0.6426\n",
      "Epoch 135, CIFAR-10 Batch 3:  Loss =  0.147817    Accuracy =  0.6338\n",
      "Epoch 135, CIFAR-10 Batch 4:  Loss =  0.139712    Accuracy =  0.6376\n",
      "Epoch 135, CIFAR-10 Batch 5:  Loss =  0.132409    Accuracy =  0.6374\n",
      "Epoch 136, CIFAR-10 Batch 1:  Loss =  0.151203    Accuracy =  0.6466\n",
      "Epoch 136, CIFAR-10 Batch 2:  Loss =  0.13452    Accuracy =  0.64\n",
      "Epoch 136, CIFAR-10 Batch 3:  Loss =  0.142145    Accuracy =  0.6414\n",
      "Epoch 136, CIFAR-10 Batch 4:  Loss =  0.125724    Accuracy =  0.6486\n",
      "Epoch 136, CIFAR-10 Batch 5:  Loss =  0.132241    Accuracy =  0.639\n",
      "Epoch 137, CIFAR-10 Batch 1:  Loss =  0.149848    Accuracy =  0.6456\n",
      "Epoch 137, CIFAR-10 Batch 2:  Loss =  0.151369    Accuracy =  0.6352\n",
      "Epoch 137, CIFAR-10 Batch 3:  Loss =  0.135286    Accuracy =  0.6448\n",
      "Epoch 137, CIFAR-10 Batch 4:  Loss =  0.124871    Accuracy =  0.6392\n",
      "Epoch 137, CIFAR-10 Batch 5:  Loss =  0.132632    Accuracy =  0.6264\n",
      "Epoch 138, CIFAR-10 Batch 1:  Loss =  0.135986    Accuracy =  0.6456\n",
      "Epoch 138, CIFAR-10 Batch 2:  Loss =  0.14915    Accuracy =  0.637\n",
      "Epoch 138, CIFAR-10 Batch 3:  Loss =  0.121087    Accuracy =  0.6438\n",
      "Epoch 138, CIFAR-10 Batch 4:  Loss =  0.1269    Accuracy =  0.6308\n",
      "Epoch 138, CIFAR-10 Batch 5:  Loss =  0.137617    Accuracy =  0.633\n",
      "Epoch 139, CIFAR-10 Batch 1:  Loss =  0.143212    Accuracy =  0.6446\n",
      "Epoch 139, CIFAR-10 Batch 2:  Loss =  0.12511    Accuracy =  0.635\n",
      "Epoch 139, CIFAR-10 Batch 3:  Loss =  0.117201    Accuracy =  0.6468\n",
      "Epoch 139, CIFAR-10 Batch 4:  Loss =  0.120278    Accuracy =  0.6426\n",
      "Epoch 139, CIFAR-10 Batch 5:  Loss =  0.126115    Accuracy =  0.6378\n",
      "Epoch 140, CIFAR-10 Batch 1:  Loss =  0.139954    Accuracy =  0.6518\n",
      "Epoch 140, CIFAR-10 Batch 2:  Loss =  0.144054    Accuracy =  0.6344\n",
      "Epoch 140, CIFAR-10 Batch 3:  Loss =  0.121277    Accuracy =  0.6366\n",
      "Epoch 140, CIFAR-10 Batch 4:  Loss =  0.123171    Accuracy =  0.6388\n",
      "Epoch 140, CIFAR-10 Batch 5:  Loss =  0.124654    Accuracy =  0.6396\n",
      "Epoch 141, CIFAR-10 Batch 1:  Loss =  0.149937    Accuracy =  0.648\n",
      "Epoch 141, CIFAR-10 Batch 2:  Loss =  0.134651    Accuracy =  0.6468\n",
      "Epoch 141, CIFAR-10 Batch 3:  Loss =  0.127688    Accuracy =  0.6466\n",
      "Epoch 141, CIFAR-10 Batch 4:  Loss =  0.122437    Accuracy =  0.642\n",
      "Epoch 141, CIFAR-10 Batch 5:  Loss =  0.12311    Accuracy =  0.6416\n",
      "Epoch 142, CIFAR-10 Batch 1:  Loss =  0.132103    Accuracy =  0.6452\n",
      "Epoch 142, CIFAR-10 Batch 2:  Loss =  0.120167    Accuracy =  0.648\n",
      "Epoch 142, CIFAR-10 Batch 3:  Loss =  0.121026    Accuracy =  0.6452\n",
      "Epoch 142, CIFAR-10 Batch 4:  Loss =  0.119652    Accuracy =  0.6432\n",
      "Epoch 142, CIFAR-10 Batch 5:  Loss =  0.117395    Accuracy =  0.6432\n",
      "Epoch 143, CIFAR-10 Batch 1:  Loss =  0.118363    Accuracy =  0.6454\n",
      "Epoch 143, CIFAR-10 Batch 2:  Loss =  0.153416    Accuracy =  0.6334\n",
      "Epoch 143, CIFAR-10 Batch 3:  Loss =  0.120819    Accuracy =  0.6448\n",
      "Epoch 143, CIFAR-10 Batch 4:  Loss =  0.122276    Accuracy =  0.6452\n",
      "Epoch 143, CIFAR-10 Batch 5:  Loss =  0.11817    Accuracy =  0.6302\n",
      "Epoch 144, CIFAR-10 Batch 1:  Loss =  0.137473    Accuracy =  0.6494\n",
      "Epoch 144, CIFAR-10 Batch 2:  Loss =  0.133169    Accuracy =  0.6514\n",
      "Epoch 144, CIFAR-10 Batch 3:  Loss =  0.115183    Accuracy =  0.6362\n",
      "Epoch 144, CIFAR-10 Batch 4:  Loss =  0.108982    Accuracy =  0.6436\n",
      "Epoch 144, CIFAR-10 Batch 5:  Loss =  0.105549    Accuracy =  0.6398\n",
      "Epoch 145, CIFAR-10 Batch 1:  Loss =  0.131177    Accuracy =  0.6416\n",
      "Epoch 145, CIFAR-10 Batch 2:  Loss =  0.121717    Accuracy =  0.6448\n",
      "Epoch 145, CIFAR-10 Batch 3:  Loss =  0.107579    Accuracy =  0.6476\n",
      "Epoch 145, CIFAR-10 Batch 4:  Loss =  0.128218    Accuracy =  0.6358\n",
      "Epoch 145, CIFAR-10 Batch 5:  Loss =  0.10734    Accuracy =  0.641\n",
      "Epoch 146, CIFAR-10 Batch 1:  Loss =  0.125988    Accuracy =  0.654\n",
      "Epoch 146, CIFAR-10 Batch 2:  Loss =  0.126608    Accuracy =  0.6402\n",
      "Epoch 146, CIFAR-10 Batch 3:  Loss =  0.116444    Accuracy =  0.6422\n",
      "Epoch 146, CIFAR-10 Batch 4:  Loss =  0.11979    Accuracy =  0.643\n",
      "Epoch 146, CIFAR-10 Batch 5:  Loss =  0.0921578    Accuracy =  0.6444\n",
      "Epoch 147, CIFAR-10 Batch 1:  Loss =  0.12469    Accuracy =  0.6532\n",
      "Epoch 147, CIFAR-10 Batch 2:  Loss =  0.119203    Accuracy =  0.6444\n",
      "Epoch 147, CIFAR-10 Batch 3:  Loss =  0.107738    Accuracy =  0.6438\n",
      "Epoch 147, CIFAR-10 Batch 4:  Loss =  0.113909    Accuracy =  0.647\n",
      "Epoch 147, CIFAR-10 Batch 5:  Loss =  0.10713    Accuracy =  0.6432\n",
      "Epoch 148, CIFAR-10 Batch 1:  Loss =  0.124684    Accuracy =  0.6392\n",
      "Epoch 148, CIFAR-10 Batch 2:  Loss =  0.152838    Accuracy =  0.631\n",
      "Epoch 148, CIFAR-10 Batch 3:  Loss =  0.113356    Accuracy =  0.6472\n",
      "Epoch 148, CIFAR-10 Batch 4:  Loss =  0.113064    Accuracy =  0.64\n",
      "Epoch 148, CIFAR-10 Batch 5:  Loss =  0.125262    Accuracy =  0.6416\n",
      "Epoch 149, CIFAR-10 Batch 1:  Loss =  0.138924    Accuracy =  0.648\n",
      "Epoch 149, CIFAR-10 Batch 2:  Loss =  0.126012    Accuracy =  0.6392\n",
      "Epoch 149, CIFAR-10 Batch 3:  Loss =  0.114939    Accuracy =  0.6514\n",
      "Epoch 149, CIFAR-10 Batch 4:  Loss =  0.122015    Accuracy =  0.6432\n",
      "Epoch 149, CIFAR-10 Batch 5:  Loss =  0.116552    Accuracy =  0.6444\n",
      "Epoch 150, CIFAR-10 Batch 1:  Loss =  0.149679    Accuracy =  0.6418\n",
      "Epoch 150, CIFAR-10 Batch 2:  Loss =  0.113382    Accuracy =  0.6442\n",
      "Epoch 150, CIFAR-10 Batch 3:  Loss =  0.120068    Accuracy =  0.6428\n",
      "Epoch 150, CIFAR-10 Batch 4:  Loss =  0.124935    Accuracy =  0.6456\n",
      "Epoch 150, CIFAR-10 Batch 5:  Loss =  0.109244    Accuracy =  0.6388\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.6361615359783173\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEv1/tR5rJDs/RdQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xf6805c78d0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. That's because there are many more techniques that can be applied to your model and we recemmond that once you are done with this project, you explore!\n",
    "\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2]",
   "language": "python",
   "name": "conda-env-tensorflow2-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
